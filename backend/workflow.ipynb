{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352ba0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading .env from: c:\\Users\\Dell\\Documents\\GitHub\\DeepRetrieve\\backend\\.env\n",
      "QDRANT_URL loaded:  Yes\n",
      "QDRANT_API_KEY loaded:  Yes\n",
      "GOOGLE_API_KEY loaded:  Yes\n",
      "\n",
      " All imports successful!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import base64  # encode and decode binary data using Base64 encoding.\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "\n",
    "# Load environment variables from .env file\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Get the directory where the notebook is located and load .env\n",
    "notebook_dir = Path.cwd()\n",
    "env_path = notebook_dir / \".env\"\n",
    "\n",
    "load_dotenv(dotenv_path=env_path, override=True)\n",
    "\n",
    "print(f\"Loading .env from: {env_path}\")\n",
    "print(f\"QDRANT_URL loaded: {' Yes' if os.getenv('QDRANT_URL') else ' No'}\")\n",
    "print(f\"QDRANT_API_KEY loaded: {' Yes' if os.getenv('QDRANT_API_KEY') else ' No'}\")\n",
    "print(f\"GOOGLE_API_KEY loaded: {' Yes' if os.getenv('GOOGLE_API_KEY') else ' No'}\")\n",
    "\n",
    "# PDF Processing\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "# Table Extraction\n",
    "import camelot\n",
    "\n",
    "# Image Processing\n",
    "from PIL import Image\n",
    "\n",
    "# ML/Embeddings\n",
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "# Vector Database\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models as qdrant_models\n",
    "from qdrant_client.http.models import Distance, VectorParams, PointStruct, BinaryQuantization, BinaryQuantizationConfig\n",
    "\n",
    "import google.generativeai as genai\n",
    "\n",
    "print(\"\\n All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6148a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "# Qdrant Cloud Configuration\n",
    "QDRANT_URL = os.getenv(\"QDRANT_URL\")\n",
    "QDRANT_API_KEY = os.getenv(\"QDRANT_API_KEY\")\n",
    "COLLECTION_NAME = \"multimodal_rag\"\n",
    "\n",
    "# CLIP Configuration  \n",
    "CLIP_MODEL_NAME = \"openai/clip-vit-base-patch32\"\n",
    "EMBEDDING_DIM = 512\n",
    "\n",
    "# RAG Configuration\n",
    "TOP_K = 3\n",
    "CHUNK_SIZE = 500\n",
    "CHUNK_OVERLAP = 50\n",
    "\n",
    "# Output Paths\n",
    "OUTPUT_FOLDER = \"extracted_content\"\n",
    "IMAGES_FOLDER = os.path.join(OUTPUT_FOLDER, \"images\")\n",
    "TABLES_FOLDER = os.path.join(OUTPUT_FOLDER, \"tables\")\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(IMAGES_FOLDER, exist_ok=True)\n",
    "os.makedirs(TABLES_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "355a95b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CLIP model...\n",
      "CLIP model loaded!\n",
      "Initializing Gemini...\n",
      "Gemini initialized!\n",
      "Connecting to Qdrant Cloud...\n",
      "CLIP model loaded!\n",
      "Initializing Gemini...\n",
      "Gemini initialized!\n",
      "Connecting to Qdrant Cloud...\n",
      "Qdrant Cloud connected!\n",
      "Qdrant Cloud connected!\n"
     ]
    }
   ],
   "source": [
    "def init_clip_model():\n",
    "    \"\"\"Initialize CLIP model and processor\"\"\"\n",
    "    print(\"Loading CLIP model...\")\n",
    "    model = CLIPModel.from_pretrained(CLIP_MODEL_NAME)\n",
    "    processor = CLIPProcessor.from_pretrained(CLIP_MODEL_NAME)\n",
    "    print(\"CLIP model loaded!\")\n",
    "    return model, processor\n",
    "\n",
    "def init_qdrant_client():\n",
    "    \"\"\"Initialize Qdrant Cloud client\"\"\"\n",
    "    print(\"Connecting to Qdrant Cloud...\")\n",
    "    client = QdrantClient(\n",
    "        url=QDRANT_URL,\n",
    "        api_key=QDRANT_API_KEY,\n",
    "    )\n",
    "    print(\"Qdrant Cloud connected!\")\n",
    "    return client\n",
    "\n",
    "def init_gemini():\n",
    "    \"\"\"Initialize Gemini model\"\"\"\n",
    "    print(\"Initializing Gemini...\")\n",
    "    genai.configure(api_key=GOOGLE_API_KEY)\n",
    "    model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "    print(\"Gemini initialized!\")\n",
    "    return model\n",
    "\n",
    "# Initialize all models\n",
    "clip_model, clip_processor = init_clip_model()\n",
    "gemini_model = init_gemini()\n",
    "qdrant_client = init_qdrant_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04407467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF Extraction Functions\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"Extract all text from a PDF file\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    full_text = []\n",
    "    \n",
    "    for page in doc:\n",
    "        text = page.get_text(\"text\")\n",
    "        full_text.append(text)\n",
    "    \n",
    "    doc.close()\n",
    "    return \"\\n\".join(full_text)\n",
    "\n",
    "\n",
    "def extract_images_from_pdf(pdf_path: str, output_folder: str = IMAGES_FOLDER) -> List[Dict]:\n",
    "    \"\"\"Extract all images from a PDF file with metadata\"\"\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    doc = fitz.open(pdf_path)\n",
    "    \n",
    "    extracted_images = []\n",
    "    pdf_name = Path(pdf_path).stem\n",
    "    \n",
    "    for page_index, page in enumerate(doc):\n",
    "        image_list = page.get_images(full=True)\n",
    "        \n",
    "        for img_index, img in enumerate(image_list):\n",
    "            xref = img[0]\n",
    "            base_image = doc.extract_image(xref)\n",
    "            image_bytes = base_image[\"image\"]\n",
    "            image_ext = base_image[\"ext\"]\n",
    "            \n",
    "            # Save image\n",
    "            image = Image.open(io.BytesIO(image_bytes))\n",
    "            image_filename = f\"{pdf_name}_page{page_index}_img{img_index}.{image_ext}\"\n",
    "            image_path = os.path.join(output_folder, image_filename)\n",
    "            image.save(image_path)\n",
    "            \n",
    "            extracted_images.append({\n",
    "                \"path\": image_path,\n",
    "                \"page\": page_index,\n",
    "                \"index\": img_index,\n",
    "                \"source_pdf\": pdf_path\n",
    "            })\n",
    "    \n",
    "    doc.close()\n",
    "    return extracted_images\n",
    "\n",
    "\n",
    "def extract_tables_from_pdf(pdf_path: str, output_folder: str = TABLES_FOLDER) -> List[Dict]:\n",
    "    \"\"\"Extract tables from PDF using Camelot and convert to text format\"\"\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    extracted_tables = []\n",
    "    pdf_name = Path(pdf_path).stem\n",
    "    \n",
    "    try:\n",
    "        # Try 'lattice' method first (for tables with borders)\n",
    "        tables = camelot.read_pdf(pdf_path, pages=\"all\", flavor=\"lattice\")\n",
    "        \n",
    "        # If no tables found, try 'stream' method (for borderless tables)\n",
    "        if len(tables) == 0:\n",
    "            tables = camelot.read_pdf(pdf_path, pages=\"all\", flavor=\"stream\")\n",
    "        \n",
    "        for i, table in enumerate(tables):\n",
    "            # Get the DataFrame\n",
    "            df = table.df\n",
    "            \n",
    "            # Convert table to markdown format for better LLM understanding\n",
    "            table_markdown = df.to_markdown(index=False)\n",
    "            \n",
    "            # Also save as CSV\n",
    "            csv_path = os.path.join(output_folder, f\"{pdf_name}_table_{i}.csv\")\n",
    "            table.to_csv(csv_path)\n",
    "            \n",
    "            # Create a text representation with context\n",
    "            table_text = f\"TABLE {i+1} (Page {table.page}):\\n{table_markdown}\"\n",
    "            \n",
    "            extracted_tables.append({\n",
    "                \"table_index\": i,\n",
    "                \"page\": table.page,\n",
    "                \"content\": table_text,\n",
    "                \"csv_path\": csv_path,\n",
    "                \"accuracy\": table.accuracy,\n",
    "                \"source_pdf\": pdf_path\n",
    "            })\n",
    "            \n",
    "        print(f\"   Extracted {len(tables)} tables\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Table extraction failed: {e}\")\n",
    "    \n",
    "    return extracted_tables\n",
    "\n",
    "\n",
    "def chunk_text(text: str, chunk_size: int = CHUNK_SIZE, overlap: int = CHUNK_OVERLAP) -> List[str]:\n",
    "    \"\"\"Split text into overlapping chunks\"\"\"\n",
    "    if not text.strip():\n",
    "        return []\n",
    "    \n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    \n",
    "    for i in range(0, len(words), chunk_size - overlap):\n",
    "        chunk = \" \".join(words[i:i + chunk_size])\n",
    "        if chunk.strip():\n",
    "            chunks.append(chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "def process_pdf(pdf_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"Process a PDF and extract text chunks, images, and tables\"\"\"\n",
    "    print(f\"Processing: {pdf_path}\")\n",
    "    \n",
    "    # Extract text and chunk it\n",
    "    raw_text = extract_text_from_pdf(pdf_path)\n",
    "    text_chunks = chunk_text(raw_text)\n",
    "    \n",
    "    # Extract images\n",
    "    images = extract_images_from_pdf(pdf_path)\n",
    "    \n",
    "    # Extract tables\n",
    "    tables = extract_tables_from_pdf(pdf_path)\n",
    "    \n",
    "    result = {\n",
    "        \"pdf_path\": pdf_path,\n",
    "        \"text_chunks\": text_chunks,\n",
    "        \"images\": images,\n",
    "        \"tables\": tables,\n",
    "        \"total_chunks\": len(text_chunks),\n",
    "        \"total_images\": len(images),\n",
    "        \"total_tables\": len(tables)\n",
    "    }\n",
    "    \n",
    "    print(f\"   ‚úÖ Extracted {len(text_chunks)} text chunks, {len(images)} images, {len(tables)} tables\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fccb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility: Index Multiple PDFs from a folder\n",
    "def index_folder(folder_path: str, collection_name: str = COLLECTION_NAME):\n",
    "    \"\"\"Index all PDFs in a folder\"\"\"\n",
    "    pdf_files = list(Path(folder_path).glob(\"*.pdf\"))\n",
    "    \n",
    "    print(f\"Found {len(pdf_files)} PDF files in {folder_path}\")\n",
    "    \n",
    "    total_chunks = 0\n",
    "    total_images = 0\n",
    "    \n",
    "    for pdf_path in pdf_files:\n",
    "        result = index_pdf(str(pdf_path), collection_name)\n",
    "        total_chunks += result[\"text_chunks\"]\n",
    "        total_images += result[\"images\"]\n",
    "    \n",
    "    print(f\"\\n Indexing complete!\")\n",
    "    print(f\"   Total text chunks: {total_chunks}\")\n",
    "    print(f\"   Total images: {total_images}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "be8b371f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Embedding functions defined!\n"
     ]
    }
   ],
   "source": [
    "# Embedding Functions (CLIP)\n",
    "\n",
    "def embed_text(text: str) -> List[float]:\n",
    "    \"\"\"Embed text using CLIP\"\"\"\n",
    "    inputs = clip_processor(\n",
    "        text=text,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=77  # CLIP's max token length\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        features = clip_model.get_text_features(**inputs)\n",
    "        # Normalize embeddings to unit vector\n",
    "        features = features / features.norm(dim=-1, keepdim=True)\n",
    "        return features.squeeze().tolist()\n",
    "\n",
    "\n",
    "def embed_image(image_input) -> List[float]:\n",
    "    \"\"\"Embed image using CLIP. Accepts file path or PIL Image\"\"\"\n",
    "    if isinstance(image_input, str):\n",
    "        image = Image.open(image_input).convert(\"RGB\")\n",
    "    elif isinstance(image_input, Image.Image):\n",
    "        image = image_input.convert(\"RGB\")\n",
    "    else:\n",
    "        raise ValueError(\"Input must be a file path or PIL Image\")\n",
    "    \n",
    "    inputs = clip_processor(images=image, return_tensors=\"pt\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        features = clip_model.get_image_features(**inputs)\n",
    "        # Normalize embeddings to unit vector\n",
    "        features = features / features.norm(dim=-1, keepdim=True)\n",
    "        return features.squeeze().tolist()\n",
    "\n",
    "\n",
    "def embed_image_base64(base64_string: str) -> List[float]:\n",
    "    \"\"\"Embed a base64 encoded image using CLIP\"\"\"\n",
    "    image_bytes = base64.b64decode(base64_string)\n",
    "    image = Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n",
    "    return embed_image(image)\n",
    "\n",
    "\n",
    "print(\" Embedding functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2b6ffb51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Qdrant functions defined!\n"
     ]
    }
   ],
   "source": [
    "# Qdrant Vector Database Functions\n",
    "\n",
    "def create_collection(collection_name: str = COLLECTION_NAME, recreate: bool = False):\n",
    "    \"\"\"Create a Qdrant collection with Binary Quantization for efficient storage\"\"\"\n",
    "    \n",
    "    if recreate and qdrant_client.collection_exists(collection_name=collection_name):\n",
    "        print(f\"Deleting existing collection: {collection_name}\")\n",
    "        qdrant_client.delete_collection(collection_name)\n",
    "    \n",
    "    if not qdrant_client.collection_exists(collection_name=collection_name):\n",
    "        qdrant_client.create_collection(\n",
    "            collection_name=collection_name,\n",
    "            vectors_config=VectorParams(\n",
    "                size=EMBEDDING_DIM,\n",
    "                distance=Distance.COSINE,\n",
    "                on_disk=True  # Store vectors on disk for large collections\n",
    "            ),\n",
    "            quantization_config=BinaryQuantization(\n",
    "                binary=BinaryQuantizationConfig(\n",
    "                    always_ram=True  # Keep quantized vectors in RAM for fast search\n",
    "                )\n",
    "            ),\n",
    "        )\n",
    "        print(f\" Created collection: {collection_name} (with Binary Quantization)\")\n",
    "    else:\n",
    "        print(f\" Collection '{collection_name}' already exists\")\n",
    "\n",
    "\n",
    "def add_text_to_qdrant(\n",
    "    text_chunks: List[str],\n",
    "    source_pdf: str,\n",
    "    collection_name: str = COLLECTION_NAME\n",
    ") -> int:\n",
    "    \"\"\"Add text chunks to Qdrant with embeddings\"\"\"\n",
    "    points = []\n",
    "    \n",
    "    for i, chunk in enumerate(text_chunks):\n",
    "        embedding = embed_text(chunk)\n",
    "\n",
    "        \"\"\"\n",
    "        In Qdrant, every entry in the vector database is called a point\n",
    "        A point contains three things:\n",
    "            id - a unique integer\n",
    "            vector - the embedding (list of floats)\n",
    "            payload - a dictionary of metadata\n",
    "\n",
    "        PointStruct is the Qdrant SDK class that represents one point.\n",
    "        \"\"\"\n",
    "\n",
    "        point = PointStruct(\n",
    "            id=hash(f\"{source_pdf}_{i}\") % (2**63),  # Unique ID using hashing\n",
    "            vector=embedding, # The embedding vector\n",
    "            payload={  # Metadata about the chunk\n",
    "                \"type\": \"text\",\n",
    "                \"content\": chunk,\n",
    "                \"source\": source_pdf,\n",
    "                \"chunk_index\": i\n",
    "            }\n",
    "        )\n",
    "        points.append(point)\n",
    "    \n",
    "    if points:\n",
    "        qdrant_client.upsert(collection_name=collection_name, points=points)\n",
    "    \n",
    "    return len(points)\n",
    "\n",
    "\n",
    "def add_images_to_qdrant(\n",
    "    images: List[Dict],\n",
    "    collection_name: str = COLLECTION_NAME\n",
    ") -> int:\n",
    "    \"\"\"Add images to Qdrant with embeddings\"\"\"\n",
    "    points = []\n",
    "    \n",
    "    for img_info in images:\n",
    "        image_path = img_info[\"path\"]\n",
    "        \n",
    "        try:\n",
    "            embedding = embed_image(image_path)\n",
    "            \n",
    "            # Read and encode image for storage\n",
    "            with open(image_path, \"rb\") as f:\n",
    "                image_base64 = base64.b64encode(f.read()).decode()\n",
    "            \n",
    "            point = PointStruct(\n",
    "                id=hash(image_path) % (2**63),\n",
    "                vector=embedding,\n",
    "                payload={\n",
    "                    \"type\": \"image\",\n",
    "                    \"path\": image_path,\n",
    "                    \"image_base64\": image_base64,\n",
    "                    \"source\": img_info[\"source_pdf\"],\n",
    "                    \"page\": img_info[\"page\"]\n",
    "                }\n",
    "            )\n",
    "            points.append(point)\n",
    "        except Exception as e:\n",
    "            print(f\" Failed to process image {image_path}: {e}\")\n",
    "    \n",
    "    if points:\n",
    "        qdrant_client.upsert(collection_name=collection_name, points=points)\n",
    "    \n",
    "    return len(points)\n",
    "\n",
    "\n",
    "def add_tables_to_qdrant(\n",
    "    tables: List[Dict],\n",
    "    collection_name: str = COLLECTION_NAME\n",
    ") -> int:\n",
    "    \"\"\"Add extracted tables to Qdrant with embeddings (embedded as markdown text)\"\"\"\n",
    "    points = []\n",
    "    \n",
    "    for table_info in tables:\n",
    "        try:\n",
    "            # Embed the markdown representation of the table\n",
    "            table_content = table_info[\"content\"]\n",
    "            embedding = embed_text(table_content)\n",
    "            \n",
    "            point = PointStruct(\n",
    "                id=hash(f\"{table_info['source_pdf']}_table_{table_info['page']}_{table_info['table_index']}\") % (2**63),\n",
    "                vector=embedding,\n",
    "                payload={\n",
    "                    \"type\": \"table\",\n",
    "                    \"content\": table_content,  # Store markdown for LLM context\n",
    "                    \"csv_path\": table_info[\"csv_path\"],\n",
    "                    \"source\": table_info[\"source_pdf\"],\n",
    "                    \"page\": table_info[\"page\"],\n",
    "                    \"table_index\": table_info[\"table_index\"]\n",
    "                }\n",
    "            )\n",
    "            points.append(point)\n",
    "        except Exception as e:\n",
    "            print(f\" Failed to process table from page {table_info.get('page', '?')}: {e}\")\n",
    "    \n",
    "    if points:\n",
    "        qdrant_client.upsert(collection_name=collection_name, points=points)\n",
    "    \n",
    "    return len(points)\n",
    "\n",
    "\n",
    "def index_pdf(pdf_path: str, collection_name: str = COLLECTION_NAME):\n",
    "    \"\"\"Index a PDF into Qdrant (text + images + tables)\"\"\"\n",
    "    # Process PDF\n",
    "    pdf_data = process_pdf(pdf_path)\n",
    "    \n",
    "    # Add text chunks\n",
    "    text_count = add_text_to_qdrant(\n",
    "        pdf_data[\"text_chunks\"],\n",
    "        pdf_path,\n",
    "        collection_name\n",
    "    )\n",
    "    print(f\" Indexed {text_count} text chunks\")\n",
    "    \n",
    "    # Add images\n",
    "    image_count = add_images_to_qdrant(\n",
    "        pdf_data[\"images\"],\n",
    "        collection_name\n",
    "    )\n",
    "    print(f\" Indexed {image_count} images\")\n",
    "    \n",
    "    # Add tables\n",
    "    table_count = add_tables_to_qdrant(\n",
    "        pdf_data[\"tables\"],\n",
    "        collection_name\n",
    "    )\n",
    "    print(f\" Indexed {table_count} tables\")\n",
    "\n",
    "    return {\"text_chunks\": text_count, \"images\": image_count, \"tables\": table_count}\n",
    "\n",
    "print(\" Qdrant functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "89ccb3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Retrieval functions defined!\n"
     ]
    }
   ],
   "source": [
    "# Retrieval Functions\n",
    "\n",
    "def search_similar(\n",
    "    query: str,\n",
    "    top_k: int = TOP_K,\n",
    "    collection_name: str = COLLECTION_NAME,\n",
    "    content_type: str = None  # \"text\", \"image\", \"table\", or None for all\n",
    ") -> List[Dict]:\n",
    "    \"\"\"Search for similar content in Qdrant\"\"\"\n",
    "    \n",
    "    # Embed the query\n",
    "    query_embedding = embed_text(query)\n",
    "    \n",
    "    # Build filter if content type specified\n",
    "    query_filter = None\n",
    "    if content_type:\n",
    "        query_filter = {\n",
    "            \"must\": [{\"key\": \"type\", \"match\": {\"value\": content_type}}]\n",
    "        }\n",
    "    \n",
    "    # Search\n",
    "    results = qdrant_client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=query_embedding,\n",
    "        limit=top_k,\n",
    "        query_filter=query_filter\n",
    "    )\n",
    "    \n",
    "    # Format results\n",
    "    formatted_results = []\n",
    "    for result in results:\n",
    "        formatted_results.append({\n",
    "            \"score\": result.score,\n",
    "            \"type\": result.payload.get(\"type\"),\n",
    "            \"content\": result.payload.get(\"content\"),\n",
    "            \"image_base64\": result.payload.get(\"image_base64\"),\n",
    "            \"source\": result.payload.get(\"source\"),\n",
    "            \"page\": result.payload.get(\"page\"),\n",
    "            \"path\": result.payload.get(\"path\"),\n",
    "            \"csv_path\": result.payload.get(\"csv_path\"),  # For tables\n",
    "            \"table_index\": result.payload.get(\"table_index\")  # For tables\n",
    "        })\n",
    "    \n",
    "    return formatted_results\n",
    "\n",
    "\n",
    "def search_by_image(\n",
    "    image_input,\n",
    "    top_k: int = TOP_K,\n",
    "    collection_name: str = COLLECTION_NAME\n",
    ") -> List[Dict]:\n",
    "    \"\"\"Search using an image as query\"\"\"\n",
    "    \n",
    "    # Embed the image\n",
    "    query_embedding = embed_image(image_input)\n",
    "    \n",
    "    # Search\n",
    "    results = qdrant_client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=query_embedding,\n",
    "        limit=top_k\n",
    "    )\n",
    "    \n",
    "    # Format results\n",
    "    formatted_results = []\n",
    "    for result in results:\n",
    "        formatted_results.append({\n",
    "            \"score\": result.score,\n",
    "            \"type\": result.payload.get(\"type\"),\n",
    "            \"content\": result.payload.get(\"content\"),\n",
    "            \"image_base64\": result.payload.get(\"image_base64\"),\n",
    "            \"source\": result.payload.get(\"source\"),\n",
    "            \"page\": result.payload.get(\"page\"),\n",
    "            \"csv_path\": result.payload.get(\"csv_path\"),\n",
    "            \"table_index\": result.payload.get(\"table_index\")\n",
    "        })\n",
    "    \n",
    "    return formatted_results\n",
    "\n",
    "\n",
    "print(\" Retrieval functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d4ce80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini LLM functions defined!\n"
     ]
    }
   ],
   "source": [
    "# Gemini LLM Functions\n",
    "\n",
    "def prepare_context_for_llm(search_results: List[Dict]) -> Tuple[str, List]:\n",
    "    \"\"\"Prepare retrieved context for Gemini (text + images + tables)\"\"\"\n",
    "    \n",
    "    text_context = []\n",
    "    images_for_llm = []\n",
    "    \n",
    "    for i, result in enumerate(search_results):\n",
    "        if result[\"type\"] == \"text\":\n",
    "            text_context.append(f\"[Source {i+1} - Text]: {result['content']}\")\n",
    "        \n",
    "        elif result[\"type\"] == \"table\":\n",
    "            # Tables are stored as markdown - include directly in text context\n",
    "            text_context.append(f\"[Source {i+1} - Table (page {result.get('page', 'unknown')})]: \\n{result['content']}\")\n",
    "        \n",
    "        elif result[\"type\"] == \"image\" and result.get(\"image_base64\"):\n",
    "            # Decode base64 to PIL Image for Gemini\n",
    "            image_bytes = base64.b64decode(result[\"image_base64\"])\n",
    "            image = Image.open(io.BytesIO(image_bytes))\n",
    "            images_for_llm.append(image)\n",
    "            text_context.append(f\"[Source {i+1} - Image]: Refer to Image {len(images_for_llm)} (from page {result.get('page', 'unknown')})\")\n",
    "    \n",
    "    context_text = \"\\n\\n\".join(text_context)\n",
    "    return context_text, images_for_llm\n",
    "\n",
    "\n",
    "def generate_response(\n",
    "    query: str,\n",
    "    search_results: List[Dict],\n",
    "    include_images: bool = True\n",
    ") -> str:\n",
    "    \"\"\"Generate response using Gemini with multimodal context\"\"\"\n",
    "    \n",
    "    # Prepare context\n",
    "    context_text, images = prepare_context_for_llm(search_results)\n",
    "    \n",
    "    # Build prompt\n",
    "    prompt = f\"\"\"You are an intelligent document assistant specialized in answering questions using retrieved context from a knowledge base.\n",
    "\n",
    "## Your Task\n",
    "Answer the user's question accurately using ONLY the information provided in the context below. The context may include:\n",
    "- **Text excerpts** from PDF documents\n",
    "- **Tables** (structured data in markdown format) extracted from documents\n",
    "- **Images** (charts, diagrams, figures) extracted from documents\n",
    "\n",
    "## Instructions\n",
    "1. **Analyze all provided context** - text, tables, and images carefully\n",
    "2. **For tables**: Parse the markdown table structure to extract specific data, values, and relationships\n",
    "3. **For images**: Describe what you see and extract relevant information (data from charts, text from diagrams)\n",
    "4. **Synthesize information** from multiple sources when needed\n",
    "6. **If information is insufficient**: Clearly state what's missing rather than guessing\n",
    "7. **Format your answer** clearly with proper structure when appropriate\n",
    "\n",
    "## Retrieved Context\n",
    "{context_text}\n",
    "\n",
    "## User Question\n",
    "{query}\n",
    "\n",
    "## Answer\n",
    "Provide a comprehensive, accurate answer based on the context above:\"\"\"\n",
    "    \n",
    "    # Generate response\n",
    "    if include_images and images:\n",
    "        # Multimodal generation with images\n",
    "        content = [prompt] + images\n",
    "        response = gemini_model.generate_content(content)\n",
    "    else:\n",
    "        # Text-only generation\n",
    "        response = gemini_model.generate_content(prompt)\n",
    "    \n",
    "    return response.text\n",
    "\n",
    "\n",
    "print(\"Gemini LLM functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e25296fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG pipeline ready!\n"
     ]
    }
   ],
   "source": [
    "# Main RAG Pipeline\n",
    "\n",
    "def rag_query(\n",
    "    query: str,\n",
    "    top_k: int = TOP_K,\n",
    "    include_images: bool = True,\n",
    "    collection_name: str = COLLECTION_NAME\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Main RAG pipeline: Search -> Retrieve -> Generate\n",
    "    \n",
    "    Args:\n",
    "        query: User's question\n",
    "        top_k: Number of results to retrieve\n",
    "        include_images: Whether to include images in context\n",
    "        collection_name: Qdrant collection to search\n",
    "    \n",
    "    Returns:\n",
    "        Dict with answer, sources, and metadata\n",
    "    \"\"\"\n",
    "    print(f\"üîç Query: {query}\")\n",
    "    \n",
    "    # Step 1: Retrieve relevant content\n",
    "    print(\"   Searching for relevant content...\")\n",
    "    search_results = search_similar(query, top_k=top_k, collection_name=collection_name)\n",
    "    \n",
    "    if not search_results:\n",
    "        return {\n",
    "            \"answer\": \"No relevant information found in the knowledge base.\",\n",
    "            \"sources\": [],\n",
    "            \"num_results\": 0\n",
    "        }\n",
    "    \n",
    "    # Step 2: Generate response with Gemini\n",
    "    print(\"   Generating response with Gemini...\")\n",
    "    answer = generate_response(query, search_results, include_images=include_images)\n",
    "    \n",
    "    # Step 3: Prepare sources for reference\n",
    "    sources = []\n",
    "    for result in search_results:\n",
    "        sources.append({\n",
    "            \"type\": result[\"type\"],\n",
    "            \"score\": round(result[\"score\"], 4),\n",
    "            \"source\": result[\"source\"],\n",
    "            \"preview\": result[\"content\"][:200] if result[\"content\"] else \"[Image]\"\n",
    "        })\n",
    "    \n",
    "    print(\"Response generated!\")\n",
    "    \n",
    "    return {\n",
    "        \"answer\": answer,\n",
    "        \"sources\": sources,\n",
    "        \"num_results\": len(search_results)\n",
    "    }\n",
    "\n",
    "\n",
    "def display_results(result: Dict):\n",
    "    \"\"\"Pretty print RAG results\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìù ANSWER:\")\n",
    "    print(\"=\"*60)\n",
    "    print(result[\"answer\"])\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"üìö SOURCES ({result['num_results']} results):\")\n",
    "    print(\"=\"*60)\n",
    "    for i, source in enumerate(result[\"sources\"], 1):\n",
    "        print(f\"\\n{i}. [{source['type'].upper()}] Score: {source['score']}\")\n",
    "        print(f\"   Source: {source['source']}\")\n",
    "        print(f\"   Preview: {source['preview'][:100]}...\")\n",
    "\n",
    "\n",
    "print(\"RAG pipeline ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f74e0373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Created collection: multimodal_rag (with Binary Quantization)\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Create Collection\n",
    "\n",
    "create_collection(COLLECTION_NAME, recreate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a8df9364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: data/attention.pdf\n",
      " Table extraction failed: module 'camelot' has no attribute 'read_pdf'\n",
      "   ‚úÖ Extracted 14 text chunks, 3 images, 0 tables\n",
      " Table extraction failed: module 'camelot' has no attribute 'read_pdf'\n",
      "   ‚úÖ Extracted 14 text chunks, 3 images, 0 tables\n",
      " Indexed 14 text chunks\n",
      " Indexed 14 text chunks\n",
      " Indexed 3 images\n",
      " Indexed 0 tables\n",
      " Indexed 3 images\n",
      " Indexed 0 tables\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text_chunks': 14, 'images': 3, 'tables': 0}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: Index PDF Documents\n",
    "\n",
    "pdf_path = \"data/attention.pdf\"\n",
    "\n",
    "# Index the PDF (extracts text + images and stores in Qdrant)\n",
    "index_pdf(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ac55ae3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Query: What is the main topic of the document?\n",
      "   Searching for relevant content...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_25312\\3426592614.py:22: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  results = qdrant_client.search(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Generating response with Gemini...\n",
      "Response generated!\n",
      "\n",
      "============================================================\n",
      "üìù ANSWER:\n",
      "============================================================\n",
      "The document is about the Transformer, a new network architecture based solely on attention mechanisms, dispensing with recurrence and convolutions entirely (Source 1). The paper discusses the Transformer's architecture, including scaled dot-product attention and multi-head attention (Source 3). It also presents the results of applying the Transformer to machine translation tasks, where it achieves superior quality and requires less training time compared to recurrent or convolutional neural networks (Source 1). Additionally, the document touches on the application of the Transformer to English constituency parsing (Source 1). Furthermore, some attention heads exhibit behavior related to the structure of the sentence and anaphora resolution (Source 2).\n",
      "\n",
      "\n",
      "============================================================\n",
      "üìö SOURCES (3 results):\n",
      "============================================================\n",
      "\n",
      "1. [TEXT] Score: 0.7696\n",
      "   Source: data/attention.pdf\n",
      "   Preview: Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and...\n",
      "\n",
      "2. [TEXT] Score: 0.7626\n",
      "   Source: data/attention.pdf\n",
      "   Preview: should be just - this is what we are missing , in my opinion . <EOS> <pad> The Law will never be per...\n",
      "\n",
      "3. [TEXT] Score: 0.7036\n",
      "   Source: data/attention.pdf\n",
      "   Preview: the corresponding key. 3.2.1 Scaled Dot-Product Attention We call our particular attention \"Scaled D...\n",
      "Response generated!\n",
      "\n",
      "============================================================\n",
      "üìù ANSWER:\n",
      "============================================================\n",
      "The document is about the Transformer, a new network architecture based solely on attention mechanisms, dispensing with recurrence and convolutions entirely (Source 1). The paper discusses the Transformer's architecture, including scaled dot-product attention and multi-head attention (Source 3). It also presents the results of applying the Transformer to machine translation tasks, where it achieves superior quality and requires less training time compared to recurrent or convolutional neural networks (Source 1). Additionally, the document touches on the application of the Transformer to English constituency parsing (Source 1). Furthermore, some attention heads exhibit behavior related to the structure of the sentence and anaphora resolution (Source 2).\n",
      "\n",
      "\n",
      "============================================================\n",
      "üìö SOURCES (3 results):\n",
      "============================================================\n",
      "\n",
      "1. [TEXT] Score: 0.7696\n",
      "   Source: data/attention.pdf\n",
      "   Preview: Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and...\n",
      "\n",
      "2. [TEXT] Score: 0.7626\n",
      "   Source: data/attention.pdf\n",
      "   Preview: should be just - this is what we are missing , in my opinion . <EOS> <pad> The Law will never be per...\n",
      "\n",
      "3. [TEXT] Score: 0.7036\n",
      "   Source: data/attention.pdf\n",
      "   Preview: the corresponding key. 3.2.1 Scaled Dot-Product Attention We call our particular attention \"Scaled D...\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Query the RAG System\n",
    "\n",
    "# Ask a question\n",
    "query = \"What is the main topic of the document?\"\n",
    "\n",
    "# Get RAG response\n",
    "result = rag_query(query, top_k=3)\n",
    "\n",
    "# Display results\n",
    "display_results(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6673098d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Query: Can you explain about the Scaled Dot-Product Attention?\n",
      "   Searching for relevant content...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_25312\\3426592614.py:22: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  results = qdrant_client.search(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Generating response with Gemini...\n",
      "Response generated!\n",
      "\n",
      "============================================================\n",
      "üìù ANSWER:\n",
      "============================================================\n",
      "Scaled Dot-Product Attention, as explained in Source 2, involves the following steps:\n",
      "\n",
      "1.  **Input:** The input consists of queries (Q) and keys (K) of dimension dk, and values (V) of dimension dv. These are packed into matrices.\n",
      "2.  **Dot Products:** Compute the dot products of the query with all keys (Q * KT).\n",
      "3.  **Scaling:** Divide each dot product by the square root of dk (‚àödk). This scaling mitigates the issue of large dot product magnitudes for larger dk values, which can push the softmax function into regions with extremely small gradients.\n",
      "4.  **Softmax:** Apply a softmax function to obtain the weights on the values.\n",
      "5.  **Output:** Compute the matrix of outputs as Attention(Q, K, V ) = softmax(QKT /‚àödk )V.\n",
      "\n",
      "\n",
      "============================================================\n",
      "üìö SOURCES (3 results):\n",
      "============================================================\n",
      "\n",
      "1. [TEXT] Score: 0.77\n",
      "   Source: data/attention.pdf\n",
      "   Preview: the representation dimension, k is the kernel size of convolutions and r the size of the neighborhoo...\n",
      "\n",
      "2. [TEXT] Score: 0.7544\n",
      "   Source: data/attention.pdf\n",
      "   Preview: the corresponding key. 3.2.1 Scaled Dot-Product Attention We call our particular attention \"Scaled D...\n",
      "\n",
      "3. [TEXT] Score: 0.6656\n",
      "   Source: data/attention.pdf\n",
      "   Preview: While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too...\n",
      "Response generated!\n",
      "\n",
      "============================================================\n",
      "üìù ANSWER:\n",
      "============================================================\n",
      "Scaled Dot-Product Attention, as explained in Source 2, involves the following steps:\n",
      "\n",
      "1.  **Input:** The input consists of queries (Q) and keys (K) of dimension dk, and values (V) of dimension dv. These are packed into matrices.\n",
      "2.  **Dot Products:** Compute the dot products of the query with all keys (Q * KT).\n",
      "3.  **Scaling:** Divide each dot product by the square root of dk (‚àödk). This scaling mitigates the issue of large dot product magnitudes for larger dk values, which can push the softmax function into regions with extremely small gradients.\n",
      "4.  **Softmax:** Apply a softmax function to obtain the weights on the values.\n",
      "5.  **Output:** Compute the matrix of outputs as Attention(Q, K, V ) = softmax(QKT /‚àödk )V.\n",
      "\n",
      "\n",
      "============================================================\n",
      "üìö SOURCES (3 results):\n",
      "============================================================\n",
      "\n",
      "1. [TEXT] Score: 0.77\n",
      "   Source: data/attention.pdf\n",
      "   Preview: the representation dimension, k is the kernel size of convolutions and r the size of the neighborhoo...\n",
      "\n",
      "2. [TEXT] Score: 0.7544\n",
      "   Source: data/attention.pdf\n",
      "   Preview: the corresponding key. 3.2.1 Scaled Dot-Product Attention We call our particular attention \"Scaled D...\n",
      "\n",
      "3. [TEXT] Score: 0.6656\n",
      "   Source: data/attention.pdf\n",
      "   Preview: While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too...\n"
     ]
    }
   ],
   "source": [
    "query = \"Can you explain about the Scaled Dot-Product Attention?\"\n",
    "\n",
    "# Get RAG response\n",
    "result = rag_query(query, top_k=3)\n",
    "\n",
    "# Display results\n",
    "display_results(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "97cdd63b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Query: Can you explain images related to attention mechanism?\n",
      "   Searching for relevant content...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_25312\\3426592614.py:22: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  results = qdrant_client.search(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Generating response with Gemini...\n",
      "Response generated!\n",
      "\n",
      "============================================================\n",
      "üìù ANSWER:\n",
      "============================================================\n",
      "Source 1 mentions Figure 4 and Figure 5, both related to attention heads. Figure 4 illustrates two attention heads involved in anaphora resolution, specifically focusing on the word \"its.\" It shows full attentions for head 5 and isolated attentions from the word \"its\" for attention heads 5 and 6, noting the sharpness of the attentions for this word. Figure 5 provides examples of attention heads exhibiting behavior related to sentence structure, showcasing different tasks learned by different heads from the encoder self-attention at layer 5 of 6.\n",
      "\n",
      "Source 3 refers to Figure 2, which illustrates \"Scaled Dot-Product Attention\" on the left and \"Multi-Head Attention\" on the right, explaining that multi-head attention consists of several attention layers running in parallel.\n",
      "\n",
      "\n",
      "============================================================\n",
      "üìö SOURCES (3 results):\n",
      "============================================================\n",
      "\n",
      "1. [TEXT] Score: 0.8045\n",
      "   Source: data/attention.pdf\n",
      "   Preview: should be just - this is what we are missing , in my opinion . <EOS> <pad> The Law will never be per...\n",
      "\n",
      "2. [TEXT] Score: 0.7921\n",
      "   Source: data/attention.pdf\n",
      "   Preview: the representation dimension, k is the kernel size of convolutions and r the size of the neighborhoo...\n",
      "\n",
      "3. [TEXT] Score: 0.7735\n",
      "   Source: data/attention.pdf\n",
      "   Preview: representations of its input and output without using sequence- aligned RNNs or convolution. In the ...\n",
      "Response generated!\n",
      "\n",
      "============================================================\n",
      "üìù ANSWER:\n",
      "============================================================\n",
      "Source 1 mentions Figure 4 and Figure 5, both related to attention heads. Figure 4 illustrates two attention heads involved in anaphora resolution, specifically focusing on the word \"its.\" It shows full attentions for head 5 and isolated attentions from the word \"its\" for attention heads 5 and 6, noting the sharpness of the attentions for this word. Figure 5 provides examples of attention heads exhibiting behavior related to sentence structure, showcasing different tasks learned by different heads from the encoder self-attention at layer 5 of 6.\n",
      "\n",
      "Source 3 refers to Figure 2, which illustrates \"Scaled Dot-Product Attention\" on the left and \"Multi-Head Attention\" on the right, explaining that multi-head attention consists of several attention layers running in parallel.\n",
      "\n",
      "\n",
      "============================================================\n",
      "üìö SOURCES (3 results):\n",
      "============================================================\n",
      "\n",
      "1. [TEXT] Score: 0.8045\n",
      "   Source: data/attention.pdf\n",
      "   Preview: should be just - this is what we are missing , in my opinion . <EOS> <pad> The Law will never be per...\n",
      "\n",
      "2. [TEXT] Score: 0.7921\n",
      "   Source: data/attention.pdf\n",
      "   Preview: the representation dimension, k is the kernel size of convolutions and r the size of the neighborhoo...\n",
      "\n",
      "3. [TEXT] Score: 0.7735\n",
      "   Source: data/attention.pdf\n",
      "   Preview: representations of its input and output without using sequence- aligned RNNs or convolution. In the ...\n"
     ]
    }
   ],
   "source": [
    "query = \"Can you explain images related to attention mechanism?\"\n",
    "\n",
    "# Get RAG response\n",
    "result = rag_query(query, top_k=3)\n",
    "\n",
    "# Display results\n",
    "display_results(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gpu)",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
